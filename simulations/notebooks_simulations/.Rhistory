0.4*M*d
M
D
d
p
sample(1:M*d, p*M*d)
1:M*d
M*d
X_tmp = rep(0, M*d)
prop_1 = seq(0, 1, 0.1)
for (p in prop_1){
print(p)
index_ones = sample(seq(1, M*d, 1), p*M*d)
print(index_ones)
X_tmp[index_ones] = 1
X_test = matrix(X_tmp, nrow=M)
print(X_test)
SI = getStability(X_test)$stability
print(SI)
}
prop_1 = seq(0, 1, 0.1)
for (p in prop_1){
print(p)
index_ones = sample(seq(1, M*d, 1), p*M*d)
print(index_ones)
X_tmp = rep(0, M*d)
X_tmp[index_ones] = 1
X_test = matrix(X_tmp, nrow=M)
print(X_test)
SI = getStability(X_test)$stability
print(SI)
}
d = 2 # number of features
M = 10 # number of bootstrap replicates
## case 1: when stability index undefined -- X all zeros or all ones (Nogueria2018 p.13)
X_all_missed = matrix(rep(0, M*d), nrow=M) # since K_bar = 0, thus SI undefined
getStability(X_all_missed)$stability
X_all_selected = matrix(rep(1, M*d), nrow=M) # since K_bar = d = 3, thus SI underfined
getStability(X_all_selected)$stability
rep(c(0,1), each=M*d/2)
matrix(rep(c(0,1), each=M*d/2), nrow=M)
Z_half_zero_one = matrix(rep(c(0,1), each=M*d/2), nrow=M) # one column (feature) all zero, the other all one
getStability(Z_half_zero_one)$stability
rep(c(1,2), 3)
d = 10
i = 2
sample(seq(1, d, 1), i)
d_ones = sample(seq(1, d, 1), i)
d_ones = sample(seq(1, d, 1), i)
Z_tmp = matrix(rep(0, M*d), nrow=M)
Z_tmp[, d_ones] = 1
Z_tmp
d = 10
for (i in 1:d){
d_ones = sample(seq(1, d, 1), i)
Z_tmp = matrix(rep(0, M*d), nrow=M)
Z_tmp[, d_ones] = 1
SI = getStability(Z_tmp)$stability
print(i)
print(paste('SI:', SI, sep=''))
}
d = 10
for (i in 1: (d-1)){
d_ones = sample(seq(1, d, 1), i)
Z_tmp = matrix(rep(0, M*d), nrow=M)
Z_tmp[, d_ones] = 1
SI = getStability(Z_tmp)$stability
print(i)
print(paste('SI:', SI, sep=''))
}
d_alt = rep(c(0, 1), M/2)
d_alt
matrix(rep(d_alt, d), ncol=d)
Z_alt = matrix(rep(d_alt, d), ncol=d)
getStability(Z_alt)$stability
-1 / (M-1)
getStability(matrix(c(rep(0, M/2), rep(1, M/2)), ncol=d))$stability
matrix(c(rep(0, M/2), rep(1, M/2)), ncol=d)
c(rep(0, M/2), rep(1, M/2))
d_alt_2 = c(rep(0, M/2), rep(1, M/2))
Z_alt_2 = matrix(rep(d_alt_2, d), ncol=d)
getStability(Z_alt_2)$stability
M = 1000
d_alt = rep(c(0, 1), M/2)
Z_alt = matrix(rep(d_alt, d), ncol=d)
getStability(Z_alt)$stability
M = 1000000
d_alt = rep(c(0, 1), M/2)
Z_alt = matrix(rep(d_alt, d), ncol=d)
getStability(Z_alt)$stability
M = 10000
d_alt = rep(c(0, 1), M/2)
Z_alt = matrix(rep(d_alt, d), ncol=d)
getStability(Z_alt)$stability
p = c(0.1, 0.2, 0.7)
si = -sum(p * log(p))
si
p = c(0.1, 0.4, 0.5)
si = -sum(p * log(p))
si
p = c(0.1, 0.4, 0.4, 0.1)
si = -sum(p * log(p))
si
980/437
437/980
d_alt = rep(c(0, 1), M/2)
Z_alt = matrix(rep(d_alt, d), ncol=d)
M = 10
d = 10
d_alt = rep(c(0, 1), M/2)
Z_alt = matrix(rep(d_alt, d), ncol=d)
Z_alt
d_alt_2 = c(rep(0, M/2), rep(1, M/2))
Z_alt_2 = matrix(rep(d_alt_2, d), ncol=d)
Z_alt_2
M = 10000
d_alt = rep(c(0, 1), M/2)
Z_alt = matrix(rep(d_alt, d), ncol=d)
getStability(Z_alt)$stability
sum(Z_alt_2)/M
M
sum(Z_alt_2)/10
d
sum(Z_alt_2)/(10*10)
2728 + 396 * 3
2628 + 396 * 3
396*3
396*6
1188 + 62 + 5 + 25
62 + 1 + 2376 + 173 + 1 + 2
62 + 1 + 173 + 1
237 + 1188
1425*2.75/100
getwd
dir = '~/Study'
paste0(dir, '/', 'sim_data.Rdata', sep='')
N = 100
R_corr = 0.1
paste0(dir, '/', paste(paste('sim_dat', 'N', N, 'R_corr', sim_param$R_corr, 'Complete',
sim_param$N_T_mean/sim_param$N_T_max, sep='_'), '.Rdata', sep=''), sep='')
paste0(dir, '/', paste(paste('sim_dat', 'N', N, 'R_corr', sim_param$R_corr, 'Complete',
100, sep='_'), '.Rdata', sep=''), sep='')
paste0(dir, '/', paste(paste('sim_dat', 'N', N, 'R_corr', 0.1, 'Complete',
100, sep='_'), '.Rdata', sep=''), sep='')
paste(paste('sim_dat', 'N', N, 'R_corr', sim_param$R_corr, 'Complete',
sim_param$N_T_mean/sim_param$N_T_max, sep='_'), '.Rdata', sep='')
paste(paste('sim_dat', 'N', N, 'R_corr', 0.1, 'Complete',
100, sep='_'), '.Rdata', sep='')
pwd
pwd()
getwd()
4800 + 1900
4800 +3400
40*5*5*4
40*8*5*4
6700-3500
6700-3500
3300 + 200 + 550
6700-4050
200 + 400 + 400 + 100
setwd("~/Study/thesis/Bayesian/mfpca/sim_barnacle/sim_N100_C100_test")
library(parallel)
library(rstan)
options(mc.cores = parallel::detectCores())
Nsamples = 1000
Nchains = 3
Ncores=Nchains
set.seed(31)
load("sim_dat_N_100_R_corr_0.1_Complete_1.Rdata", dat <- new.env())
nsims=500
R_true = dat$sim_param$R_corr
R_mean_list = R_q1_list = R_q9_list = R_q025_list = R_q975_list = list()
wid_80 = wid_95 = rep(0, nsims) # width of credible intervals
cov_80 = cov_95 = rep(0, nsims) # coverage of credible intervals
for (i in 1:nsims){
print(paste('nsims', i))
model_file = "../mfpca_zhou_fixed_R_simplest.stan"
smod = stan_model(model_file)
pca_data <- list(N = dat$sim_param$N, P = dat$sim_param$P, K = dat$sim_param$Q, Q = (dat$sim_param$nknots + 4),
theta_mu = dat$sim_param$params[[7]], sigma_eps = dat$sim_param$params[[5]],
Theta = dat$sim_param$params[[6]], D = diag(dat$sim_param$D_true), alpha = t(dat$ALPHA[[i]]))
fit_R_simplest = sampling(smod, data= pca_data, iter=Nsamples, chains=Nchains, cores=Ncores, init="random")
results_80 = summary(fit_R_simplest, pars = c("R[1,2]"), probs = c(0.1, 0.9))$summary
results_95 = summary(fit_R_simplest, pars = c("R[1,2]"), probs = c(0.025, 0.975))$summary
R_mean_list[[i]] = results_80 [, 'mean']
R_q1_list[[i]] = results_80[, '10%']
R_q9_list[[i]] = results_80[, '90%']
R_q025_list[[i]] = results_95[, '2.5%']
R_q975_list[[i]] = results_95[, '97.5%']
# check coverage probability & width of credible intervals
wid_80[i] = R_q9_list[[i]] - R_q1_list[[i]]
wid_95[i] =  R_q975_list[[i]] - R_q025_list[[i]]
if (R_true <= R_q9_list[[i]] & R_true >= R_q1_list[[i]]){
cov_80[i] = 1
}
if (R_true <= R_q975_list[[i]] & R_true >= R_q025_list[[i]]){
cov_95[i] = 1
}
}
dat$sim_param$N
34.37 + 37.73 + 16.69 + 34.35
15.28+15.85+23.44+8.5+40.71+29.96+38.52
172.26+123.14
require(gamair)
install.packages('gamair')
require(gamair)
data(engine)
attach(engine)
plot(size, wear, xlab='engine capacity', ylab='wear index')
tf <- function(x, xj, j){df<- xj*0; dj[j] <-1; approx(xj, dj, x)$y}
tf.X <- function(x, xj){
## tent function basis matrix given data x and knot sequence xj
nk <- length(xj); n <- length(x)
X <- matrix(NA, n, nk)
for (j in 1:nk) X[,j] <- tf(x, xj, j)
X
}
# use a rank k =6 basis, with knots evendly spread over the range of the size data
sj <- seq(min(size), max(size), length=6)
# use a rank k =6 basis, with knots evendly spread over the range of the size data
sj <- seq(min(size), max(size), length=6) ## generate knots
X <- tf.x (size, sj) ## get model matrix
X <- tf.X (size, sj) ## get model matrix
tf <- function(x, xj, j){
## generate jth tent function from set defined by knots xj
dj<- xj*0; dj[j] <-1
approx(xj, dj, x)$y
}
tf.X <- function(x, xj){
## tent function basis matrix given data x and knot sequence xj
nk <- length(xj); n <- length(x)
X <- matrix(NA, n, nk)
for (j in 1:nk) X[,j] <- tf(x, xj, j)
X
}
# use a rank k =6 basis, with knots evendly spread over the range of the size data
sj <- seq(min(size), max(size), length=6) ## generate knots
X <- tf.X (size, sj) ## get model matrix
b <- lm(wear ~ X - 1) ## fit model
s <- seq(min(size), max(size), length=200) ## prediction data
Xp <- tf.x(s, sj) ## prediction matrix
Xp <- tf.X(s, sj) ## prediction matrix
plot(size, wear) ## plot data
lines(s, Xp %*% coef(b)) ## overlay estimated f
## lambda controls the model fit
sj <- seq(min(size), max(size), length=20) ## knots
b <- prs.fit(wear, size, sj, 2) ## penalized fit
plot(size, wear) ## plot data
prs.fit <- function(y, x, xj, sp){
X <- tf.X(x, xj) ## model matrix
D <- diff(diag(length(xj)), differences=2) ## sqrt penalty
X <- rbind(X, sqrt(sp) * D) ## augmented model matrix
y <- c(y, rep(0, nrow(D))) ## augmented data
lm(y ~ X - 1) ## penalized least squares fit
}
## lambda controls the model fit
sj <- seq(min(size), max(size), length=20) ## knots
b <- prs.fit(wear, size, sj, 2) ## penalized fit
plot(size, wear) ## plot data
Xp <- tf.X(s, sj) ## prediction matrix
lines(s, Xp %*% coef(b)) ## plot the smooth
rho = seq(-9, 11, length=90)
n <- length(wear)
v <- rep(NA, 90)
for (i in 1:90){ ## loop through smoothing params
b <- prs.fit(wear, size, sj, exp(rho[i])) ## fit model
trF <- sum(influence(b)$hat[1:n]) ## extract EDF
rss <- sum((wear - fitted(b)[1:n]) ^2) ## residual SS
V[i] <- n*rss/(n-trF)^2 ## GCV score
}
rho = seq(-9, 11, length=90)
n <- length(wear)
V <- rep(NA, 90)
for (i in 1:90){ ## loop through smoothing params
b <- prs.fit(wear, size, sj, exp(rho[i])) ## fit model
trF <- sum(influence(b)$hat[1:n]) ## extract EDF
rss <- sum((wear - fitted(b)[1:n]) ^2) ## residual SS
V[i] <- n*rss/(n-trF)^2 ## GCV score
for (i in 1:90){ ## loop through smoothing params
b <- prs.fit(wear, size, sj, exp(rho[i])) ## fit model
trF <- sum(influence(b)$hat[1:n]) ## extract EDF
rss <- sum((wear - fitted(b)[1:n]) ^2) ## residual SS
V[i] <- n*rss/(n-trF)^2 ## GCV score
}
pplot(rho, V, type='l', xlab=expression(log(lambda)), main='GCV score')
sp <- exp(rho[V == min(V)]) ## extract optimal sp
b <- prs.fit(wear, size, sj, sp) ## re-fit
plot(size, wear, main='GCV optimal fit')
lines(s, Xp %*% coef(b))
plot(rho, V, type='l', xlab=expression(log(lambda)), main='GCV score')
plot(rho, V, type='l', xlab=expression(log(lambda)), main='GCV score')
V
rho = seq(-9, 11, length=90)
n <- length(wear)
V <- rep(NA, 90)
for (i in 1:90){ ## loop through smoothing params
b <- prs.fit(wear, size, sj, exp(rho[i])) ## fit model
trF <- sum(influence(b)$hat[1:n]) ## extract EDF
rss <- sum((wear - fitted(b)[1:n]) ^2) ## residual SS
V[i] <- n*rss/(n-trF)^2 ## GCV score
}
V
plot(rho, V, type='l', xlab=expression(log(lambda)), main='GCV score')
sp <- exp(rho[V == min(V)]) ## extract optimal sp
b <- prs.fit(wear, size, sj, sp) ## re-fit
plot(size, wear, main='GCV optimal fit')
lines(s, Xp %*% coef(b))
sp
X0 <- tf.X(size, sj) ## X in original parameterization
D <- rbind(0, 0, diff(diag(20), difference=2))
diag(D) <- 1 ## augmented D
X <- t(backsolve(t(D), t(X0))) ## re-parametrized X
Z <- X[, -c(1,2)]; X<- X[, 1:2] ## mixed model matrices
## estimate smoothing and variance parameters
m <- optim(c(0,0), llm, method='BFGS', X=X, Z=Z, y=wear) ## Bayesian model
b <- attr(llm(m$par, X, Z, wear), 'b') ## extract coefficients
## plot results
plot(size, wear)
Xp1 <- t(backsolve(t(D), t(Xp))) ## re-parameterized pred.mat.
lines(s, Xp1 %*% as.numeric(b), col='grey', lwd=2)
library(nlme)
g <- factor(rep(1, nrow(X))) ## dummy factor
m <- lme(wear ~ X - 1, random=list(g=pdIdent(~ Z-1)))
lines(s, Xp1 %*% as.numeric(coef(m))) ## and to plot
tf.XD <- function(x, xk, cmx=NULL, m=2){
## get X and D subject to constraint
nk <- length(xk)
X <- tf.X(x, xk)[, -nk] ## basis matrix
D <- diff(diag(nk), differences=m)[, -nk] ## root penalty
if (is.null(cmx)) cmx <- colMeans(X)
X <- sweep(X, 2, cmx) ## subtract cmx from columns
list (X=X, D=D, cmx=cmx)
}
am.fit <- function (y,x,v,sp,k=10) {
## setup basis and penalties...
xk <- seq(min(x), max(x), length=k)
xdx <- tf.XD(x, xk)
vk <- seq(min(v), max(v), length=k)
xdv <- tf.XD(v, vk)
## create augmented model matrix and response ...
nD <- nrow(xdx$D) * 2
sp <- sqrt(sp)
X <- cbind(c(rep(1, nrow(xdx$X)), rep(0, nD)),
rbind(xdx$X, sp[1]*xdx$D, xdv$D*0),
rbind(xdv$X, xdx$D*0, sp[2]*xdv$D))
y1 <- c(y, rep(0, nD))
## fit model ..
b <- lm(y1 ~ X - 1)
## compute some useful quantities ...
n <- length(y)
trA <- sum(influence(b)$hat[1:n]) ## EDF
rsd <- y - fitted(b)[1:n] ## residuals
rss <- sum(rsd^2) ## residual SS
sig.hat <- rss/(n-trA) ## residual variance
gcv <- sig.hat*n/(n-trA) ## GCV score
Vb <- vcov(b)*sig.hat/summary(b)$sigma^2 ## coef cov matrix
## return fitted model ...
list(b=coef(b), Vb=Vb, edf=trA, gcv=gcv, fitted=fitted(b)[1:n],
rsd=rsd, xk=list(xk, vk), cmx=list(xdx$cmx, xdv$cmx))
}
am.gcv <- function(lsp, y, x, v, k){
## function suitable for GCV optimization by optim
am.fit(y, x, v, exp(lsp), k)$gcv
}
require(mgcv)
## mgcv package
library(mgcv)
data(trees)
ct1 <- gam(Volume ~ s(Height) + s(Girth), family = Gamma(link=log), data=trees)
ct1
plot(ct1, residuals=TRUE)
## use penalized cubic regression splines
ct2 <- gam(Volume ~ s(Height, bs='cr') + s(Girth, bs='cr'),
family=Gamma(link=log), data=trees)
cts
ct2
## change dimension of basis (i.e. max df; default is 10)
ct3 <- gam(Volume ~ s(Height, bs='cr') + s(Girth, bs='cr',k=20),
family=Gamma(link=log), data=trees)
ct3
## adjust gamma param to avoid overfitting of GCV
ct4 <- gam(Volume ~ s(Height, bs='cr') + s(Girth, bs='cr'),
family=Gamma(link=log), data=trees, gamma=1.4)
ct4
## same model, different code
ct5 <- gam(Volume ~ s(Height, Girth,k=25),
family=Gamma(link=log), data=trees)
ct5
plot(ct5, too.far=0.15)
## user tensor product smooth ('te')
ct6 <- gam(Volume ~ te(Height, Girth,k=25),
family=Gamma(link=log), data=trees)
## user tensor product smooth ('te')
ct6 <- gam(Volume ~ te(Height, Girth,k=5),
family=Gamma(link=log), data=trees)
ct6
plot(ct6, too.far=0.15)
## mix smooth and parametric model
gam(Volume ~ Height + s(Girth), family = Gamma(link=log), data=trees)
labels=c('small', 'medium', 'large')
## change Height to be categorical
trees$Hclass <- factor(floor(trees$Height/10) -5,
labels=c('small', 'medium', 'large'))
ct7 <- gam(Volume ~ Hclass + s(Girth), family=Gamma(link=log), data=trees)
par(mfrow=c(1,2)); plot(ct7,all.terms=TRUE)
anova(ct7)
AIC(ct7)
summary(ct7)
22.5*4*4
FEAST - a scalable algorithm for quantifying the origins of complex microbial communities
-----------------------
300 * 0.85
300 * 0.95
114.32-107.7
19.05 + 5.40
9.91 + 21.32 - 22.04 + 20.94
391.60/5
library(vegan)
5000/12
667.5+445
667.5+445
300*0.85
350*0.85
250*0.85
325*0.85
25*0.85
100*0.85
299.99*0.9
299.99*0.95
30*0.85
25*0.85
160*0.85
25*0.85
175*0.85
295*0.85
50.44+48.92
41.88+42.61
41.88+42.61+48.87+42.61+41.88+20.49
1.24+123.11
246.05+42.98+8.99+25.74+24.45+15.16+19.83+38.58+25.25+9.89+15.05
179.92+25.67+114.32+45.21+126.98+109.01+74.85+21.48+6.62+179.92+25.67+7.63+19.99
19.99+126.98+109.01+7.63+74.85+114.32+21.48+6.62+179.92+25.67
8.28+44.24+8.28+55.91+2.96+41.38+54.60
45.16+46.69
686.47+215.65+91.85
99.36+19.67+471.97+60
410+113+1010+705
25.67+179.92+114.32+21.48+74.85-107.7+15.69+109.01+25.74+45.21+126.98-20.46+19.99+22.27
3262.33+123.11
1200+400+3500
5100+100
5200+300
+1000
1200+400+3500+100
5200+300
3800-5500
1700-500
1200+400+1000+3500+100+300
1000+200+400+800+3500+100+300
15*0.85
setwd("~/Study/thesis/machineLearning/stability/simulations_2020/code_method")
mf = read.csv('../microbiome_data/mapping_sleep_alpha.txt', sep='\t', row.names='X.SampleID')
dim(mf) # 599 * 79
mf_sub = mf[, c('AMFVT', 'AMFVT_C1', 'AMAMPT', 'AMAMPT_C1', 'AMPHIT',
'AMPHIT_15SD', 'PQBADSLP', 'SLEEPHRS', 'PQPSQI', 'PQPEFFCY')]
vars_cts = c('AMFVT', 'AMAMPT', 'AMPHIT', 'PQPSQI', 'PQPEFFCY')
mf_sub[vars_cts] = lapply(mf_sub[vars_cts], as.numeric)
## use rarefied OTU table
library(biomformat)
taxa_biom = read_biom("../microbiome_data/mros_deblur_otus_unrare.biom")
taxa_biom = t(as.matrix(biom_data(taxa_biom)))
dim(taxa_biom) # 599 * 70125
## log transform data
taxa_otus = taxa_biom + 0.5 # replace zero counts by the maximum rounding error 0.5
taxa_otus = diag(1/rowSums(taxa_otus)) %*% taxa_otus # convert counts to relative abundance
rowSums(taxa_otus) # expect all 1
taxa_otus = log(taxa_otus) # log transformed
rownames(taxa_otus) = rownames(taxa_biom)
sample_ids = intersect(rownames(taxa_otus), rownames(mf_sub))
sample_ids = sort(sample_ids) # in alphabetical order
length(sample_ids)
taxa_otus = taxa_otus[sample_ids, ]
mf_sub = mf_sub[sample_ids, ]
dim(taxa_otus)[1] == dim(mf_sub)[1]
dim(taxa_otus) # 599 * 70125
source('cv_method.R')
#### nothing was selected with compositional lasso ###
fit.compLasso <- cons_lasso_cv(datx=taxa_otus, y=mf_sub$AMFVT, seednum=31)
fit.compLasso$coef.chosen
fit.compLasso$coef.chosen
fit.compLasso
fit.compLasso$MSE
seq(10, 50, 10)
setwd("~/Study/thesis/machineLearning/stability/simulations_2020/notebooks")
source('../code_method/getStability.R')
source('../code_method/cv_method.R')
sim_file = '../sim_data/sim_toeplitz_corr0.3P_50_N_500.RData';
load(sim_file, dat <- new.env())
mtry.grid=seq(10, 50, 10)
pdf('../figures_sim/figure_mse_stab_rf_independent_P50_N500.pdf')
for (i in sample(1:100, 2)){
print(i)
sub = dat$sim_array[[i]]
results = randomForest_double_cv(datx=sub$Z, y=sub$Y, mtry.grid=mtry.grid, fold.cv=5)
print('plot')
plot(results$mtry.grid, results$MSE.value, ylim=c(0,5),
col='green', xlab='lambda', ylab='MSE/STAB',
main=paste('rf_toe_P50_N500_random_', i, sep=''))
points(results$mtry.grid, results$STAB.value, col='red')
legend('topright', c('MSE', 'STAB'), col=c('green', 'red'), pch=c(1,1))
}
dev.off()
